{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\renewcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\renewcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "$$\n",
    "# Part 1: Backpropagation\n",
    "<a id=part1></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this part, we'll implement **backpropagation** and **automatic differentiation** from scratch and compare our implementations to `PyTorch`'s built in implementation (`autograd`)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Reminder: The backpropagation algorithm is at the core of training deep models. To state the problem we'll tackle in this notebook, imagine we have an L-layer MLP model, defined as\n",
    "$$\n",
    "\\hat{\\vec{y}^i} = \\vec{y}_L^i= \\varphi_L \\left(\n",
    "\\mat{W}_L \\varphi_{L-1} \\left( \\cdots\n",
    "\\varphi_1 \\left( \\mat{W}_1 \\vec{x}^i + \\vec{b}_1 \\right)\n",
    "\\cdots \\right)\n",
    "+ \\vec{b}_L \\right),\n",
    "$$\n",
    "\n",
    "a pointwise loss function $\\ell(\\vec{y}, \\hat{\\vec{y}})$ and an empirical loss over our entire data set,\n",
    "$$\n",
    "L(\\vec{\\theta}) = \\frac{1}{N} \\sum_{i=1}^{N} \\ell(\\vec{y}^i, \\hat{\\vec{y}^i}) + R(\\vec{\\theta})\n",
    "$$\n",
    "\n",
    "where $\\vec{\\theta}$ is a vector containing all network parameters, e.g.\n",
    "$\\vec{\\theta} = \\left[ \\mat{W}_{1,:}, \\vec{b}_1, \\dots,  \\mat{W}_{L,:}, \\vec{b}_L \\right]$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Calculus tells us that as long as we know the derivatives of all the functions \"along the way\"\n",
    "($\\varphi_i(\\cdot),\\ \\ell(\\cdot,\\cdot),\\ R(\\cdot)$)\n",
    "we can use the **chain rule** to calculate the derivative \n",
    "of the loss with respect to any one of the parameter vectors.\n",
    "Note that if the loss $L(\\vec{\\theta})$ is scalar (which is usually the case), the gradient of a parameter\n",
    "will have the same shape as the parameter itself (matrix/vector/tensor of same dimensions).\n",
    "\n",
    "For deep models that are a composition of many functions, calculating the gradient of each parameter by hand and implementing hard-coded gradient derivations quickly becomes infeasible.\n",
    "Additionally, such code makes models hard to change, since any change potentially requires re-derivation and re-implementation of the entire gradient function.\n",
    "\n",
    "The backpropagation algorithm, which we saw [in the lecture](https://vistalab-technion.github.io/cs236781/lecture_notes/lecture_03/#error-backpropagation), provides us with a effective method of applying the **chain rule** recursively so that we can implement gradient calculations of arbitrarily deep or complex models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To do this, we'll define a `Layer` class. Here's the API of this class:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In other words, a `Layer` can be anything: a layer, an activation function, a loss function or generally *any computation that we know how to derive a gradient for*.\n",
    "\n",
    "Each `Layer` must define a `forward()` function and a `backward()` function.\n",
    "- The `forward()` function performs the actual calculation/operation of the block and returns an output.\n",
    "- The `backward()` function computes the gradient of the **input and parameters** as a function of the gradient of the **output**, according to the chain rule."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note that the diagram doesn't show that if the function is parametrized, i.e. $f(\\vec{x},\\vec{y})=f(\\vec{x},\\vec{y};\\vec{w})$, there are also gradients to calculate for the parameters $\\vec{w}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, since we know how to calculate the derivative of $f(\\vec{x},\\vec{y};\\vec{w})$,\n",
    "it means we know how to calculate $\\pderiv{\\vec{z}}{\\vec{x}}$, $\\pderiv{\\vec{z}}{\\vec{y}}$ and $\\pderiv{\\vec{z}}{\\vec{w}}$ .\n",
    "Thanks to the chain rule, this is all we need to calculate the gradients of the **loss** w.r.t. the input and\n",
    "parameters:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\pderiv{L}{\\vec{x}} &= \\pderiv{L}{\\vec{z}}\\cdot \\pderiv{\\vec{z}}{\\vec{x}}\\\\\n",
    "\\pderiv{L}{\\vec{y}} &= \\pderiv{L}{\\vec{z}}\\cdot \\pderiv{\\vec{z}}{\\vec{y}}\\\\\n",
    "\\pderiv{L}{\\vec{w}} &= \\pderiv{L}{\\vec{z}}\\cdot \\pderiv{\\vec{z}}{\\vec{w}}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Our `Layer`s are therefore a combination of the ideas in `Module` and `Function` and we'll implement them together,\n",
    "just to make things simpler.\n",
    "Our goal here is to create a \"poor man's autograd\": We'll use PyTorch tensors,\n",
    "but we'll calculate and store the gradients in our `Layer`s (or return them).\n",
    "The gradients we'll calculate are of the entire block, not individual operations on tensors.\n",
    "\n",
    "To test our implementation, we'll use PyTorch's `autograd`.\n",
    "\n",
    "Note that of course this method of tracking gradients is **much** more limited than what PyTorch offers. However it allows us to implement the backpropagation algorithm very simply and really see how it works."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's set up some testing instrumentation:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Layer Implementations\n",
    "<a id=part1_2></a>\n",
    "\n",
    "We'll now implement some `Layer`s that will enable us to later build an MLP model of arbitrary depth, complete with automatic differentiation.\n",
    "\n",
    "For each block, you'll first implement the `forward()` function.\n",
    "Then, you will calculate the derivative of the block by hand with respect to each of its\n",
    "input tensors and each of its parameter tensors (if any).\n",
    "Using your manually-calculated derivation, you can then implement the `backward()` function.\n",
    "\n",
    "Notice that we have intermediate Jacobians that are potentially high dimensional tensors.\n",
    "For example in the expression\n",
    "$\\pderiv{L}{\\vec{w}} = \\pderiv{L}{\\vec{z}}\\cdot \\pderiv{\\vec{z}}{\\vec{w}}$,\n",
    "the term $\\pderiv{\\vec{z}}{\\vec{w}}$ is a 4D Jacobian if both $\\vec{z}$ and $\\vec{w}$\n",
    "are 2D matrices.\n",
    "\n",
    "In order to implement the backpropagation algorithm efficiently,\n",
    "we need to implement every backward function without explicitly constructing this \n",
    "Jacobian. Instead, we're interested in directly calculating the vector-Jacobian product\n",
    "(VJP) $\\pderiv{L}{\\vec{z}}\\cdot \\pderiv{\\vec{z}}{\\vec{w}}$. \n",
    "In order to do this, you should try to figure out the gradient of the loss with respect to\n",
    "one element, e.g. $\\pderiv{L}{\\vec{w}_{1,1}}$ and extrapolate from there how to\n",
    "directly obtain the VJP."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### (Leaky) ReLU\n",
    "\n",
    "ReLU, or rectified linear unit is a very common activation function in deep learning architectures.\n",
    "In it's most standard form, as we'll implement here, it has no parameters.\n",
    "\n",
    "We'll first implement the \"leaky\" version, defined as\n",
    "\n",
    "$$\n",
    "\\mathrm{relu}(\\vec{x}) = \\max(\\alpha\\vec{x},\\vec{x}), \\ 0\\leq\\alpha<1\n",
    "$$\n",
    "\n",
    "This is similar to the ReLU activation we've seen in class, only that it has a small non-zero slope then it's input is negative.\n",
    "Note that it's not strictly differentiable, however it has sub-gradients, defined separately any positive-valued input and for negative-valued input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "N = 100\n",
    "in_features = 200\n",
    "num_classes = 10\n",
    "eps = 1e-6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now using the LeakyReLU, we can trivially define a regular ReLU block as a special case.\n",
    "\n",
    "**TODO**: Complete the implementation of the `ReLU` class in the `hw2/layers.py` module."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Sigmoid\n",
    "\n",
    "The sigmoid function $\\sigma(x)$ is also sometimes used as an activation function.\n",
    "We have also seen it previously in the context of logistic regression.\n",
    "\n",
    "The sigmoid function is defined as\n",
    "\n",
    "$$\n",
    "\\sigma(\\vec{x}) = \\frac{1}{1+\\exp(-\\vec{x})}.\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Hyperbolic Tangent\n",
    "\n",
    "The hyperbolic tangent function $\\tanh(x)$ is a common activation function used when the output should be in the range \\[-1, 1\\].\n",
    "\n",
    "The tanh function is defined as\n",
    "\n",
    "$$\n",
    "\\tanh(\\vec{x}) = \\frac{\\exp(x)-\\exp(-x)}{\\exp(x)+\\exp(-\\vec{x})}.\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Linear (fully connected) layer\n",
    "\n",
    "First, we'll implement an affine transform layer, also known as a fully connected layer.\n",
    "\n",
    "Given an input $\\mat{X}$ the layer computes,\n",
    "\n",
    "$$\n",
    "\\mat{Z} = \\mat{X} \\mattr{W}  + \\vec{b} ,~\n",
    "\\mat{X}\\in\\set{R}^{N\\times D_{\\mathrm{in}}},~\n",
    "\\mat{W}\\in\\set{R}^{D_{\\mathrm{out}}\\times D_{\\mathrm{in}}},~ \\vec{b}\\in\\set{R}^{D_{\\mathrm{out}}}.\n",
    "$$\n",
    "\n",
    "Notes:\n",
    "- We write it this way to follow the implementation conventions.\n",
    "- $N$ is the number of samples in the input (batch size). The input $\\mat{X}$ will always be a tensor containing a batch dimension first.\n",
    "- Thanks to broadcasting, $\\vec{b}$ can remain a vector even though the input $\\mat{X}$ is a matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing gradients... \n",
      "input    diff=0.000\n",
      "param#01 diff=0.000\n",
      "param#02 diff=0.000\n",
      "Comparing gradients... \n",
      "input    diff=0.000\n",
      "param#01 diff=0.000\n",
      "param#02 diff=0.000\n"
     ]
    }
   ],
   "source": [
    "# Test Linear\n",
    "out_features = 1000\n",
    "fc = layers.Linear(in_features, out_features)\n",
    "x_test = torch.randn(N, in_features)\n",
    "\n",
    "# Test forward pass\n",
    "z = fc(x_test)\n",
    "test.assertSequenceEqual(z.shape, [N, out_features])\n",
    "torch_fc = torch.nn.Linear(in_features, out_features,bias=True)\n",
    "torch_fc.weight = torch.nn.Parameter(fc.w)\n",
    "torch_fc.bias = torch.nn.Parameter(fc.b)\n",
    "test.assertTrue(torch.allclose(torch_fc(x_test), z, atol=eps))\n",
    "\n",
    "# Test backward pass\n",
    "test_block_grad(fc, x_test)\n",
    "\n",
    "# Test second backward pass\n",
    "x_test = torch.randn(N, in_features)\n",
    "z = fc(x_test)\n",
    "z = fc(x_test)\n",
    "test_block_grad(fc, x_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TODO**: Complete the implementation of the `CrossEntropyLoss` class in the `hw2/layers.py` module."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Building Models\n",
    "<a id=part1_3></a>\n",
    "\n",
    "Now that we have some working `Layer`s, we can build an MLP model of arbitrary depth and compute end-to-end gradients.\n",
    "\n",
    "First, lets copy an idea from PyTorch and implement our own version of the `nn.Sequential` `Module`.\n",
    "This is a `Layer` which contains other `Layer`s and calls them in sequence. We'll use this to build our MLP model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Complete the implementation of the `Sequential` class in the `hw2/layers.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing gradients... \n",
      "input    diff=0.000\n",
      "param#01 diff=0.000\n",
      "param#02 diff=0.000\n",
      "param#03 diff=0.000\n",
      "param#04 diff=0.000\n",
      "param#05 diff=0.000\n",
      "param#06 diff=0.000\n",
      "param#07 diff=0.000\n",
      "param#08 diff=0.000\n",
      "param#09 diff=0.000\n",
      "param#10 diff=0.000\n",
      "param#11 diff=0.000\n",
      "param#12 diff=0.000\n",
      "param#13 diff=0.000\n",
      "param#14 diff=0.000\n"
     ]
    }
   ],
   "source": [
    "# Test Sequential\n",
    "# Let's create a long sequence of layers and see\n",
    "# whether we can compute end-to-end gradients of the whole thing.\n",
    "\n",
    "seq = layers.Sequential(\n",
    "    layers.Linear(in_features, 100),\n",
    "    layers.Linear(100, 200),\n",
    "    layers.Linear(200, 100),\n",
    "    layers.ReLU(),\n",
    "    layers.Linear(100, 500),\n",
    "    layers.LeakyReLU(alpha=0.01),\n",
    "    layers.Linear(500, 200),\n",
    "    layers.ReLU(),\n",
    "    layers.Linear(200, 500),\n",
    "    layers.LeakyReLU(alpha=0.1),\n",
    "    layers.Linear(500, 1),\n",
    "    layers.Sigmoid(),\n",
    ")\n",
    "x_test = torch.randn(N, in_features)\n",
    "\n",
    "# Test forward pass\n",
    "z = seq(x_test)\n",
    "test.assertSequenceEqual(z.shape, [N, 1])\n",
    "\n",
    "# Test backward pass\n",
    "test_block_grad(seq, x_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, equipped with a `Sequential`, all we have to do is create an MLP architecture.\n",
    "We'll define our MLP with the following hyperparameters:\n",
    "- Number of input features, $D$.\n",
    "- Number of output classes, $C$.\n",
    "- Sizes of hidden layers, $h_1,\\dots,h_L$.\n",
    "\n",
    "So the architecture will be:\n",
    "\n",
    "FC($D$, $h_1$) $\\rightarrow$ ReLU $\\rightarrow$\n",
    "FC($h_1$, $h_2$) $\\rightarrow$ ReLU $\\rightarrow$\n",
    "$\\cdots$ $\\rightarrow$\n",
    "FC($h_{L-1}$, $h_L$) $\\rightarrow$ ReLU $\\rightarrow$\n",
    "FC($h_{L}$, $C$)\n",
    "\n",
    "We'll also create a sequence of the above MLP and a cross-entropy loss, since it's the gradient of the loss that we need in order to train a model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TODO**: Complete the implementation of the `MLP` class in the `hw2/layers.py` module. Ignore the `dropout` parameter for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP loss=2.3087198734283447, activation=relu\n",
      "Comparing gradients... \n",
      "input    diff=0.000\n",
      "param#01 diff=0.000\n",
      "param#02 diff=0.000\n",
      "param#03 diff=0.000\n",
      "param#04 diff=0.000\n",
      "param#05 diff=0.000\n",
      "param#06 diff=0.000\n",
      "param#07 diff=0.000\n",
      "param#08 diff=0.000\n",
      "MLP loss=2.281424045562744, activation=sigmoid\n",
      "Comparing gradients... \n",
      "input    diff=0.000\n",
      "param#01 diff=0.000\n",
      "param#02 diff=0.000\n",
      "param#03 diff=0.000\n",
      "param#04 diff=0.000\n",
      "param#05 diff=0.000\n",
      "param#06 diff=0.000\n",
      "param#07 diff=0.000\n",
      "param#08 diff=0.000\n"
     ]
    }
   ],
   "source": [
    "# Test MLP architecture\n",
    "N = 100\n",
    "in_features = 10\n",
    "num_classes = 10\n",
    "for activation in ('relu', 'sigmoid'):\n",
    "    mlp = layers.MLP(in_features, num_classes, hidden_features=[100, 50, 100], activation=activation)\n",
    "    test.assertEqual(len(mlp.sequence), 7)\n",
    "    \n",
    "    num_linear = 0\n",
    "    for b1, b2 in zip(mlp.sequence, mlp.sequence[1:]):\n",
    "        if (str(b2).lower() == activation):\n",
    "            test.assertTrue(str(b1).startswith('Linear'))\n",
    "            num_linear += 1\n",
    "            \n",
    "    test.assertTrue(str(mlp.sequence[-1]).startswith('Linear'))\n",
    "    test.assertEqual(num_linear, 3)\n",
    "\n",
    "    # Test MLP gradients\n",
    "    # Test forward pass\n",
    "    x_test = torch.randn(N, in_features)\n",
    "    labels = torch.randint(low=0, high=num_classes, size=(N,), dtype=torch.long)\n",
    "    z = mlp(x_test)\n",
    "    test.assertSequenceEqual(z.shape, [N, num_classes])\n",
    "\n",
    "    # Create a sequence of MLPs and CE loss\n",
    "    seq_mlp = layers.Sequential(mlp, layers.CrossEntropyLoss())\n",
    "    loss = seq_mlp(x_test, y=labels)\n",
    "    test.assertEqual(loss.dim(), 0)\n",
    "    print(f'MLP loss={loss}, activation={activation}')\n",
    "\n",
    "    # Test backward pass\n",
    "    test_block_grad(seq_mlp, x_test, y=labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Questions\n",
    "<a id=part2_7></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**TODO** Answer the following questions. Write your answers in the appropriate variables in the module `hw2/answers.py`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "### Question 1 \n",
    "\n",
    "Suppose we have a linear (i.e. fully-connected) layer with a weight tensor $\\mat{W}$, defined with `in_features=1024` and `out_features=512`. We apply this layer to an input tensor $\\mat{X}$ containing a batch of `N=64` samples. The output of the layer is denoted as $\\mat{Y}$.\n",
    "\n",
    "1. Consider the Jacobian tensor $\\pderiv{\\mat{Y}}{\\mat{X}}$ of the output of the layer w.r.t. the input $\\mat{X}$.\n",
    "    1. What is the shape of this tensor?\n",
    "    1. Is this Jacobian sparse (most elements zero by definition)? If so, why and which elements?\n",
    "    1. Given the gradient of the output w.r.t. some downstream scalar loss $L$, $\\delta\\mat{Y}=\\pderiv{L}{\\mat{Y}}$, do we need to materialize the above Jacobian in order to calculate the downstream gratdient w.r.t. to the input ($\\delta\\mat{X}$)? If yes, explain why; if no, show how to calcualte it without materializing the Jacobian.\n",
    "\n",
    "1. Consider the Jacobian tensor $\\pderiv{\\mat{Y}}{\\mat{W}}$ of the output of the layer w.r.t. the layer weights $\\mat{W}$. Answer questions A-C about it as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'display_answer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[109], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m display_answer(hw2\u001b[39m.\u001b[39manswers\u001b[39m.\u001b[39mpart1_q1)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'display_answer' is not defined"
     ]
    }
   ],
   "source": [
    "display_answer(hw2.answers.part1_q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_answer(hw2.answers.part1_q2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
